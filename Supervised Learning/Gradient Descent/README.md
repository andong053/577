# Gradient Descent
	
## Definition	
Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. 
This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear 
regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine 
learning courses.
	
# References
	
https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21
	

# Dataset
	
Random generated numberical data.

